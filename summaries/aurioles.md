# Causal Inference for Fairer Clinical Prediction Models

| Student | Jose Benitez-Aurioles |
| :------ | :---------------------- |
| University | Manchester |
| Lead Supervisor | Matthew Sperrin |

## Lay Summary

**Before:** 

> Is artificial intelligence always fair? More and more AI methods that aim to improve our standard of care are developed every year. Amongst these, clinical prediction models use information like age, smoking status or blood pressure to calculate a patient’s risk of having a disease or developing it in the future. This can be a valuable tool for doctors to tailor a patient’s treatment or refer them for screening. These models learn from rich data sources, like GP records or medical studies. Yet, they can sometimes carry over historical prejudices and perpetuate them. To combat this, researchers have developed measures to detect this unfair bias and ensure that these models benefit all patients, regardless of their race, gender, or socioeconomic status. Do these 'fairness tools' always work? This isn't very well understood, especially when we don't know if and how a model is unfair. In this project, we will answer this question and extend the researcher's toolkit to address current limitations. With this in mind, we will look at current lung cancer prediction models and contribute to the development of an early detection screening approach that is fairer across all patients.

**After:**

> Is artificial intelligence (AI) fair? Every year, more and more AI tools are developed to improve our standard of care. Amongst these, clinical prediction models use information like age or blood pressure, to calculate a patient’s risk of having a disease or developing it in the future. This can be a valuable tool for doctors to tailor a patient’s treatment or refer them for screening.  These models learn from sources like GP records or medical studies. Yet, they can sometimes carry over prejudices currently existing in healthcare. For example, patients from more deprived areas tend to receive worse care compared to those from wealthier ones. A model could learn to mimic this and incorrectly prioritise the treatment of richer patients. This bias is present in most real-world data, so researchers have developed ways to detect it and ensure that their predictions benefit all patients. Do these 'fairness tools' always work? This isn't very well understood. Determining when a model’s use of patient’s characteristics (like age or smoking status) is fair or unfair isn’t clear either. In this project, we explore these questions and extend the researcher's toolkit for tackling them, focusing on clinical prediction models used in lung cancer screening.
